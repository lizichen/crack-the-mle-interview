{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before asking the following questions, reserve 3~5 minutes for:\n",
    "- greet the candidate\n",
    "- briefly introduce ZG, ZO, myself, and work\n",
    "- ask the candidate for past MLE experiences\n",
    "\n",
    "After the technical questions, leave 3~5 minutes for the candidates to ask questions.\n",
    "\n",
    "If the candidate does not know `numpy` or `python`, pseudo code is _okay_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Interview Questions (40~50 minutes)\n",
    "\n",
    "## Give a general use case:\n",
    "At Zillow, we have many prediction problems to solve. For example, we need to select features from a home to predict the price it may sell for. Let's assume we have a dataset $X$ that has many features of a home; i.e., number of bedrooms, year of constructions, etc. And we need to predict a target $Y$ which presents the selling price of the home. \n",
    "\n",
    "1. What are some of the fundamental models that we can try? How to choose?\n",
    "    - data type: numerial or categorical\n",
    "    - choose baseline model first\n",
    "\n",
    "Given that *Linear Regression* is commonly used for baseline trials, let's dive deep into the details of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data 1\n",
    "Present a simple 2-D $X$ that only has one feature, for illustration purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use(['ggplot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a generic expression of the linear regression model: $$y = mX + b$$\n",
    "let\n",
    "$$m = 3$$\n",
    "$$b = 4$$\n",
    "These $m, b$ are the ground truth of the coefficients of our model, but not necessarily the coefficients of our fitted model since we will be adding noise to the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 3\n",
    "b = 4\n",
    "X = 2 * np.random.rand(100,1)\n",
    "y = m * X + (b + np.random.randn(100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 0, 15]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X, y,'b.')\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0,2,0,15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Solution: \n",
    "2. When we want to build a model, how to prepare the data?\n",
    "    - training dataset, test dataset.\n",
    "    - in matrix format, dataframes\n",
    "    \n",
    "3. What's the analytical solution for linear regression? Under what circumstance can we or can't we use such solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closed form analytical solution: (Simplified version)\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "y & = X \\theta + \\epsilon \\\\\n",
    "X^{T} y & = X^TX\\theta \\\\\n",
    "(X^TX)^{-1}X^{T} y & = \\theta \\\\\n",
    "\\text{thus, }\\ \\hat{\\theta} & = (X^TX)^{-1}X^T y\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Exceptions: Inverse of $X^TX$ may not exist\n",
    "- X is non-invertible\n",
    "- X is singular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How to write python code for the analytical solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.16797274]\n",
      " [2.80918267]]\n"
     ]
    }
   ],
   "source": [
    "# analytical solution\n",
    "X_b = np.c_[np.ones((100,1)), X] # add extra column of all 1's to X\n",
    "theta_optimal = np.linalg.inv( X_b.T.dot(X_b) ).dot(X_b.T).dot(y) # perform the linear algebra inverse, transpose, and multiplication operations\n",
    "# theta_optimal should be close to m and b respectively.\n",
    "print(theta_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "5. gradient descent math derivation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple model equation:\n",
    "$$\\hat{Y} = mX + b$$\n",
    ", where gradient $m$ and intercept $b$ are the coefficients. We want to find the optimal coefficients so that the model has the least prediction error:\n",
    "$$\\text{Error} = \\hat{Y} - Y$$\n",
    ", where $\\hat{Y}$ is the predicted target value and $Y$ is the actual ground truth value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, we want to minimize the cost function:\n",
    "$$\\text{Cost_Function} = J_{m,b} = \\frac{1}{2} \\cdot \\frac{1}{n} \\sum_{i=1}^N (\\hat{Y}_i - Y_i)^2 = \\frac{1}{2} \\cdot \\frac{1}{n} \\sum_{i=1}^N (\\text{Error}_i)^2$$\n",
    "by _gradually_ change the coefficients to its optimum:\n",
    "$$m_{new} = m_{current} - \\delta m_{current}$$\n",
    "$$b_{new} = b_{current} - \\delta b_{current}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplify the $\\text{Cost_Function}$:\n",
    "\\begin{equation} \n",
    "\\begin{split}\n",
    "J_{m,b} & = \\frac{1}{2n} \\sum_{i=1}^N (\\hat{Y}_i - Y_i)^2 \\\\\n",
    "        & = \\frac{1}{2n} \\sum_{i=1}^N (\\text{Error}_i)^2 \\\\\n",
    "        & \\sim \\text{Error}^2\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivatives:\n",
    "\\begin{equation} \n",
    "\\begin{split}\n",
    "\\frac{\\partial J_{m,b}}{\\partial m} & = 2 \\cdot \\text{Error} \\cdot \\frac{\\partial}{\\partial m} \\text{Error}\\\\\n",
    "\\frac{\\partial J_{m,b}}{\\partial b} & = 2 \\cdot \\text{Error} \\cdot \\frac{\\partial}{\\partial b} \\text{Error} \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation} \n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial m} \\text{Error} & = \\frac{\\partial}{\\partial m}(\\hat{Y} - Y) = \\frac{\\partial}{\\partial m}(mX+b - Y) = X \\\\\n",
    "\\frac{\\partial}{\\partial b} \\text{Error} & = \\frac{\\partial}{\\partial b}(\\hat{Y} - Y) = \\frac{\\partial}{\\partial b}(mX+b - Y) = 1\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Chain rule:\n",
    "\\begin{equation} \n",
    "\\begin{split}\n",
    "\\frac{\\partial J_{m,b}}{\\partial m} & = 2 \\cdot \\text{Error} \\cdot X \\cdot \\text{Learning_Rate} = \\text{Error} \\cdot X \\cdot \\alpha \\\\\n",
    "\\frac{\\partial J_{m,b}}{\\partial b} & = 2 \\cdot \\text{Error} \\cdot 1 \\cdot \\text{Learning_Rate} = \\text{Error} \\cdot \\alpha\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Thus:\n",
    "$$m_1 = m_0 - \\text{Error} \\cdot X \\cdot \\alpha$$\n",
    "$$b_1 = b_0 - \\text{Error} \\cdot \\alpha$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalized model equation:\n",
    "$$\\hat{Y} = h_{\\theta}(X)$$\n",
    ", where $\\theta$ carries all parameters of the model. $h$ is the prediction function that takes $X$ to predict $\\hat{Y}$. And similar to the above, the error can be written as:\n",
    "$$\\text{Error} = \\hat{Y} - Y = \\sum_{i=1}^N (h_\\theta(x_i) - y_i)$$\n",
    ", where $\\hat{Y}$ is the predicted target value and $Y$ is the actual ground truth value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-write the cost function with $\\theta$, and $h$:\n",
    "\n",
    "$$ \\text{Cost_Function} = J_{\\theta} = \\frac{1}{2} \\cdot \\frac{1}{n} \\sum_{i=1}^N (\\text{Error}_i)^2 = \\frac{1}{2n} \\sum_{i=1}^N (h_\\theta(x_i) - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using numerical approach; gradient descent, we gradually change $\\theta$ to its optimal:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\theta_{new} &= \\theta_{current} - \\delta \\cdot \\theta_{current} \\\\\n",
    "         &= \\theta_{current} - \\underbrace{\\alpha}_{\\text{Learning_Rate}} \\cdot \\frac{\\partial}{\\partial \\theta} J_\\theta\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get $\\frac{\\partial}{\\partial \\theta}J_\\theta$:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial \\theta} J_\\theta &= \\frac{\\partial}{\\partial \\theta} \\frac{1}{2n} \\sum_{i=1}^N (h_\\theta(x_i) - y_i)^2 \\\\\n",
    "                                          &= \\frac{1}{2n} \\sum_{i=1}^N (h_\\theta(x_i) - y_i) \\cdot 2 \\cdot \\frac{\\partial}{\\partial \\theta_j} (h_\\theta(x_i) - y_i) \\\\\n",
    "                                          &= \\frac{1}{n} \\sum_{i=1}^N (h_\\theta(x_i) - y_i) \\cdot \\frac{\\partial}{\\partial \\theta_j} (h_\\theta(x_i) - y_i) \\\\\n",
    "                                          &= \\frac{1}{n} \\sum_{i=1}^N (h_\\theta(x_i) - y) \\cdot x_i \\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore,\n",
    "$$ \\theta_1 = \\theta_0 - \\alpha \\cdot \\frac{1}{n} \\sum_{i=1}^N (h_{\\theta_0}(x_i) - y_i) \\cdot x_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Python class \n",
    "6. Design and write a Linear Regression Python class that enables training and prediction. \n",
    "    - calculate_cost\n",
    "    - gradient_descent\n",
    "    - learning_rate, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLR:\n",
    "    \n",
    "    def __init__(self, op_algorithm):\n",
    "        self.coef_ = self.cost_ = None\n",
    "        self.op_algorithm = op_algorithm\n",
    "  \n",
    "    def calculate_cost(self,\n",
    "                       X: np.ndarray, y: np.ndarray):\n",
    "        predictions = self._predict(X)\n",
    "        self.cost_ = 1/(2 * len(y)) * np.sum(np.square(predictions - y))\n",
    "\n",
    "    # base gd\n",
    "    def gradient_descent(self, X: np.ndarray, y: np.ndarray,\n",
    "                         alpha: float, iterations: int):\n",
    "        for i in range(iterations):\n",
    "            predictions = self._predict(X)\n",
    "            error = predictions - y\n",
    "            self.coef_ = self.coef_ - alpha * (1/len(y)) * X.transpose().dot(error)\n",
    "            self.calculate_cost(X, y)\n",
    "    \n",
    "    # sgd:\n",
    "    def sgd(self, X: np.ndarray, y: np.ndarray, alpha: float, iterations: int):\n",
    "        sampling_count = len(y)\n",
    "        for i in range(iterations):\n",
    "            for i in range(sampling_count):\n",
    "                random_index = np.random.randint(0, sampling_count)\n",
    "                X_i = X[random_index, :].reshape(1, X.shape[1])\n",
    "                y_i = y[random_index].reshape(1, 1)\n",
    "                prediction_i = np.dot(X_i, self.coef_)\n",
    "                self.coef_ = self.coef_ - alpha * (1/len(y)) * (X_i.transpose().dot(prediction_i - y_i))\n",
    "            self.calculate_cost(X, y)\n",
    "    \n",
    "    # minibatch gd:\n",
    "    def minibatch_gd(self, X: np.ndarray, y: np.ndarray, alpha: float, iterations: int):\n",
    "        total_length = len(y)\n",
    "        # determine mini-batch size:\n",
    "        batch_size = np.random.randint(0, total_length/3)\n",
    "        if batch_size > 6 or batch_size == 0:\n",
    "            batch_size = 6\n",
    "            \n",
    "        for i in range(iterations):\n",
    "            for i in range(0, total_length, batch_size):\n",
    "                X_i = X[i:i+batch_size, :]\n",
    "                y_i = y[i:i+batch_size, :]\n",
    "                \n",
    "                prediction_i = np.dot(X_i, self.coef_)\n",
    "                self.coef_ = self.coef_ - alpha * (1/len(y)) * (X_i.transpose().dot(prediction_i - y_i))\n",
    "            self.calculate_cost(X, y)\n",
    "    \n",
    "    def add_intercept(self, X: np.ndarray):\n",
    "        return np.c_[X, np.ones((X.shape[0], 1))]\n",
    "            \n",
    "    def fit(self, X: np.ndarray,y: np.ndarray, \n",
    "            alpha:float = 0.01, iterations:int = 1000):\n",
    "        X = self.add_intercept(X)\n",
    "        self.coef_ = np.random.randn(X.shape[1],1)\n",
    "        if self.op_algorithm == 'gd':\n",
    "            self.gradient_descent(X, y, alpha, iterations)\n",
    "        elif self.op_algorithm == 'sgd':\n",
    "            self.sgd(X, y, alpha, iterations)\n",
    "        elif self.op_algorithm == 'minibatch':\n",
    "            self.minibatch_gd(X, y, alpha, iterations)\n",
    "\n",
    "    def _predict(self, X: np.ndarray):\n",
    "        return X.dot(self.coef_)\n",
    "    \n",
    "    def predict(self, X: np.ndarray):\n",
    "        X = self.add_intercept(X)\n",
    "        return self._predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from the previous section:\n",
    "m = 3\n",
    "b = 4\n",
    "X = 2 * np.random.rand(100,1)\n",
    "y = m * X + (b + np.random.randn(100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = [3.34147488], b = [3.597596]\n",
      "cost = 0.5706598522731284\n"
     ]
    }
   ],
   "source": [
    "lr = SimpleLR('gd')\n",
    "lr.fit(X, y)\n",
    "\n",
    "theta_final = lr.coef_\n",
    "cost_final  = lr.cost_\n",
    "\n",
    "print(f'm = {theta_final[0]}, b = {theta_final[1]}')\n",
    "print(f'cost = {cost_final}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "In reality, when our training data is huge. We have to figure out how to train a model computational effectively. \n",
    "\n",
    "Difference:\n",
    "- Whole data set vs. single data point\n",
    "- SGD needs more iterations to reach minima but computationally effective. \n",
    "\n",
    "  \n",
    "7. Improve `SimpleLR` class with new `sgd` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgd:\n",
    "# run within previous class\n",
    "def sgd(self, X: np.ndarray, y: np.ndarray, alpha: float, iterations: int):\n",
    "    sampling_count = len(y)\n",
    "    for i in range(iterations):\n",
    "        for i in range(sampling_count):\n",
    "            random_index = np.random.randint(0, sampling_count)\n",
    "            X_i = X[random_index, :].reshape(1, X.shape[1])\n",
    "            y_i = y[random_index].reshape(1, 1)\n",
    "            prediction_i = np.dot(X_i, self.coef_)\n",
    "            self.coef_ = self.coef_ - alpha * (1/len(y)) * (X_i.transpose().dot(prediction_i - y_i))\n",
    "        self.calculate_cost(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = [3.3601628], b = [3.57280484]\n",
      "cost = 0.5723480794318191\n"
     ]
    }
   ],
   "source": [
    "# run a test:\n",
    "lr = SimpleLR('sgd')\n",
    "lr.fit(X, y)\n",
    "\n",
    "theta_final = lr.coef_\n",
    "cost_final  = lr.cost_\n",
    "\n",
    "print(f'm = {theta_final[0]}, b = {theta_final[1]}')\n",
    "print(f'cost = {cost_final}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent (Optional)\n",
    "Depends on the size of a mini-batch, the updates of coefficients can be less noisier than SGD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = [3.32839583], b = [3.6116646]\n",
      "cost = 0.569676177117663\n"
     ]
    }
   ],
   "source": [
    "# run a test:\n",
    "lr = SimpleLR('minibatch')\n",
    "lr.fit(X, y)\n",
    "\n",
    "theta_final = lr.coef_\n",
    "cost_final  = lr.cost_\n",
    "\n",
    "print(f'm = {theta_final[0]}, b = {theta_final[1]}')\n",
    "print(f'cost = {cost_final}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "If we overfit our training data, our predictions may not generalize well to _new_ unseen data. Introduce regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J_\\theta = \\frac{1}{2N} \\sum_{i=1}^N (h_\\theta(x_i) - y_i)^2 + \\frac{1}{2}\\cdot \\lambda ||\\theta||^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "Implement k-fold cross validation into the SimpleLR class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[3], [2]], [[7], [1]], [[8], [9]], [[10], [6]]]\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, folds=3):\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)\n",
    "    \n",
    "    fold_size = int(len(dataset) / folds)\n",
    "\n",
    "    for i in range(folds):\n",
    "        fold = list()\n",
    "        \n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "        \n",
    "    return dataset_split\n",
    "\n",
    "\n",
    "\n",
    "# test cross validation split\n",
    "seed(1)\n",
    "dataset = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\n",
    "folds = cross_validation_split(dataset, 4)\n",
    "print(folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.random.rand(100,1)\n",
    "dataset_split = cross_validation_split(dataset, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.37458473]),\n",
       " array([0.21732737]),\n",
       " array([0.29842971]),\n",
       " array([0.52054303]),\n",
       " array([0.02764166]),\n",
       " array([0.18695714]),\n",
       " array([0.44286977]),\n",
       " array([0.06603028]),\n",
       " array([0.26503477]),\n",
       " array([0.5440444]),\n",
       " array([0.41179915]),\n",
       " array([0.53937969]),\n",
       " array([0.98687981]),\n",
       " array([0.18546414]),\n",
       " array([0.71455605]),\n",
       " array([0.57040168]),\n",
       " array([0.94473119]),\n",
       " array([0.30638842]),\n",
       " array([0.03052292]),\n",
       " array([0.90925283]),\n",
       " array([0.21445817]),\n",
       " array([0.07386257]),\n",
       " array([0.1001815]),\n",
       " array([0.87052566]),\n",
       " array([0.92768182])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
