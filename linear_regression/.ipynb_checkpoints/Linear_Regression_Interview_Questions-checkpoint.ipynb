{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before asking the following questions, reserve 3~5 minutes for:\n",
    "- greet the candidate\n",
    "- briefly introduce ZG, ZO, myself, and work\n",
    "- ask the candidate for past MLE experiences\n",
    "\n",
    "After the technical questions, leave 3~5 minutes for the candidates to ask questions.\n",
    "\n",
    "If the candidate does not know `numpy` or `python`, pseudo code is _okay_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Interview Questions (40~50 minutes)\n",
    "\n",
    "## Give a general use case:\n",
    "At Zillow, we have many prediction problems to solve. For example, we need to select features from a home to predict the price it may sell for. Let's assume we have a dataset $X$ that has many features of a home; i.e., number of bedrooms, year of constructions, etc. And we need to predict a target $Y$ which presents the selling price of the home. \n",
    "\n",
    "1. What are some of the fundamental models that we can try? How to choose?\n",
    "    - data type: numerial or categorical\n",
    "    - choose baseline model first\n",
    "\n",
    "Given that *Linear Regression* is commonly used for baseline trials, let's dive deep into the details of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data 1\n",
    "Present a simple 2-D $X$ that only has one feature, for illustration purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use(['ggplot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a generic expression of the linear regression model: $$y = mX + b$$\n",
    "let\n",
    "$$m = 3$$\n",
    "$$b = 4$$\n",
    "These $m, b$ are the ground truth of the coefficients of our model, but not necessarily the coefficients of our fitted model since we will be adding noise to the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 3\n",
    "b = 4\n",
    "X = 2 * np.random.rand(100,1)\n",
    "y = m * X + (b + np.random.randn(100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 0, 15]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAESCAYAAAD5d3KwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHsRJREFUeJzt3X+0HOV93/H36v4QqFJAzsaxL8aBVC6BEieuZXxVemQori0IgfRUfMHGPtgmR22KMXFtYhOSyjiNTaMawpGbxCqm2LUN+kKIQ1wkoI6J2vRe2mvHDrY5cR3b5YdIsJDMj4jq/tr+MbvSzrJ77+zeZ5+Z2f28ztGR9tfM9472zmeeZ555plKr1RAREWlYlXcBIiJSLAoGERFJUTCIiEiKgkFERFIUDCIikqJgEBGRFAWDiIikKBhERCRFwSAiIimjeRewBF2SLSLSvcpKF1DkYGD//v15l7CkarXKgQMH8i5jWaozLNUZluoMZ2JiIshy1JUkIiIpCgYREUlRMIiISIqCQUREUhQMIiKSomAQEZEUBYOIiKQoGEREJEXBICIiKUGvfDaz24ALgafd/cyW1z4I7AB+wt2LffmgiMgQC91iuB3Y0vqkmZ0M/DPgscDrExGRwIIGg7vvAw62eelm4NfQxHgiIoXX93MMZnYR8KS7f6Pf6xIRkZXr6+yqZrYGuB54S8b3bwO2Abg71Wq1j9Wt3OjoaOFrBNUZmuoMS3UWT6VWC9u7Y2anAF9y9zPN7GeBLwOH6y+/CtgPnOXuf7PMomqadjsM1RmW6gxLdYZTn3a72PdjcPdHgJc3HpvZD4CNGpUkIlJcQc8xmNkdwBRwmpk9YWZXhly+iIj0X9AWg7u/bZnXTwm5PhERCU9XPouISIqCQUREUhQMIiKSomAQEZEUBYOIiKQoGEREJEXBICIiKQoGERFJUTCIiEiKgkFERFIUDCIikqJgEBGRFAWDiIikKBhERCRFwSAiIikKBhERSVEwiIhIioJBRERSFAwiIpKiYBARkZTRkAszs9uAC4Gn3f3M+nM7gF8EZoG/Bt7t7j8KuV4REQkndIvhdmBLy3MPAme6+2uB7wDXBV6niIgEFDQY3H0fcLDluQfcfb7+cBp4Vch1iohIWLHPMbwH2BN5nSIi0oWg5xiWYmbXA/PA55d4zzZgG4C7U61WI1XXm9HR0cLXCKozNNUZluosnijBYGZXkJyUPs/da53e5+67gF31h7UDBw7EKK9n1WqVotcIqjM01RmW6gxnYmIiyHL6HgxmtgX4EPAmdz/c7/WJiMjKhB6uegdwDlA1syeA7SSjkFYDD5oZwLS7/6uQ6xURKbuZmTGmplazadMRNm6cy7WWoMHg7m9r8/SnQ65DRGTQzMyMcemlP87cXIWxsbXs3v1MruGgK59FRHI2NbWaubkKCwsV5uYqTE2tzrUeBYOISM42bTrC2FiNkZEaY2M1Nm06kms90YariohIexs3zrF79zODeY5BRER6s3HjXO6B0KCuJBERSVEwiIhIioJBRGQAzMyMBVuWgkFEpOQa10GEomAQESm5xnUQoSgYRERKrnEdRCgKBhGRkmtcBxGKgkFEZACEvAZCwSAiIikKBhERSVEwiIhENDMzxs6da4NedxCa5koSEYmkaPdd6EQtBhGRSIp234VOFAwiIpEU7b4LnagrSUQkkqLdd6ETBYOISERFuu9CJ0GDwcxuAy4Ennb3M+vPvQzYDZwC/AAwdz8Ucr0iMU1PV9izZ22hj/iGyczMWC5H4M3rBQrfCuhG6BbD7cAngc82Pfdh4MvufqOZfbj++EOB1ysSxczMGJddNsrs7LpCjyoZFnmN8mle78jIOqDGwkKxRxp1I+jJZ3ffBxxsefpi4DP1f38G+KWQ6xSJaWpqNbOzFH5UybDIa5RPer2UYqRRN2KMSvpJd38KoP73yyOsU6QvNm06wvg4hR9VMiwao3xWrapRqdRYv34h6nqT7wEdRxqV4WK2dgp18tnMtgHbANydarWac0VLGx0dLXyNoDpD2rIFHnywxle+ssjmzTUmJ0/Iu6SXmJ6usG9fhXPPXcUb3lDs7Qkr+3/fsgU+8YlFrrlmhMXFCh/5yIlMTq5lcjLcFNQNzXVu2QL337/Avn0VNm9O1tX4d+M7MT1dqXc7wvj4Ovbune9LXf0QIxj+1sxe6e5Pmdkrgac7vdHddwG76g9rBw4ciFBe76rVKkWvEVRnaG94Q5VTT03qLFq5zX3fH/sY3Hnns4Xv717p//vjj6+lVlvH4mKF2dkae/a8yIYNLwSsMNFa54YNyZ/mx3DsO7Fnz1pmZ9exsNDfuppNTEwEWU6MrqR7gSvq/74C+OMI6xQZSs1937OzDER/93KKetFYUevKIvRw1TuAc4CqmT0BbAduBNzMrgQeAy4JuU4ROSbZGa0FYHycnndGeQ0B7UVRLxoral1ZVGq1wvZ51fbv3593DUsqS9eH6gyr6HU2durnn388Gzb8sKfPHxsCWuv78Mtqtcrevc8Wfgda9P93ONqVtOKbPxfq5LOIrFzjytpq9biezoE0d0c1HvdzZz09XSnFjKPDRJPoiUhK7L7xffsqA3cdQC/aDW3Na7irWgwi8hKXXHIYqLB16+G+H71v3pwEEFC6k7ShtLuCG8itJaVgEJGjWs8vbN16uO/rnJyspU7SAuzcOVxzUbXrvgOiduk1UzCIDIgQI4lin19oaJwXKcsdzkJrHk3W3Gpq91wMCgaRAdBuh7plS/fL6bSDClnnUuGVVzCFsJJg7jS0Na/hrgoGkQHQbofaSzD0c+x9ltZAv4OpX0K0dNrdpyGvezcoGEQGQPsd6nE9LWu5ndFyR8adXs/SGoh9UVioC/nK3NJpR8EgMgBi7VCXOzJe6vWsrYFYR8khz2eUtaXTiYJBZEDE2KEud2S81Ovdhle/p+UIeZRf5ukv2lEwiEhmyx0ZL/d61vDq5+ikRuCsX78Q9PqJMtzLOSsFg8gQWelR+HJHxqGOnPvVZ/+5zx3P9defyOIijI/XuOGGZzl0aGQgjvJDUjCIDIlQR+HLHRmHOHLuR5/9zMwY119/AvPzAMm05N/85hgnnbS44mUPGgWDyJAo08iZfvTZT02tZnGxQjL5aI1KBXbvXsPCQuegbG5h9TL8t6wUDCJDomwjZ0L32Sf3617L7CysWlXjzW8+woMPHtcxKFtbWPffv5C6Y9sgyxQMZvYHwL8ETnL3/S2vnQY8Avy+u18TvkQRCWHQRs50q/XnB3jooWROonZB2drC2revomBoMUUSDGcBX2x57WbgOeAj4coSkX4YpJEzK7VcULa2sDZvLuxNzYLLGgzT9b9TwWBmvwCcD1zl7ocC1yYiBVOmW3626nTyvdPP0Rock5Mn9HTjozLKFAzu/ldmdpAkGAAwszHgJuCbwKf6U56IFEXZZz7t5eT7sLawurmD2zSw0cwa9xO9BvgHwK+6+0LwykSkUJp3rGW801rsO9OVWTejkqaBC4DT6q2H3wS+6O5fzvJhM3s/8MtAjeRk9bvd/f91Wa9IYZS5W6UXZRvV1GrYT753o5tgmKr/fRawGVgNfCDLB83sJOB9wBnu/qKZOXAZcHsX6xcpjLJ3q/RiEHasw9o11K1uguFhYBG4EvgnwA53/16X6zrezOaANcD+Zd4vUlhlulgsJO1Yh0Pmcwzu/jzwbZLWwtPAb3fx2SeB/wA8BjwFPOvuD3RXqkhxqL86aTXt3LmWmZmxvEuRwCq1WvaxuWb2aeA9JOcHbu/ic+uBPwQuBX4E3AXc7e6fa3nfNmAbgLu/fnZ2NnNteRgdHWU+mXil0FRnWI06p6cr7NtXYfPmGpOTxRvj3ml7hqh7errCli2jzM7C+Djs3Tt/dFndLr9s/+9FNj4+DsmcHyuSuSupPjz1HGAG+EyX63kz8H13/2F9WfcA/xhIBYO77wJ21R/WDhR80HC1WqXoNYLqDK1R54YNHL0Stohlt9ue6XMjtZ7PjezZs5bZ2XUsLFSYna2xZ8+LbNjwQk/LL9v/ez+tdEDDxMREkDq6OcfwQeBU4HJ37/Yw4zFg0szWAC8C55EEjIjUxRjlFOrcSKcRSsN67iWEIg1oWDIYzOxlwFuB1wLXAje5+/RSn2nH3R82s7uBrwHzwF9wrGUgMvRi7RRCDTntNEKp7ENa81SkUF2uxfBW4AskJ5tvBj7c64rcfTuwvdfPi4RUtGsQYu0UQg45bTdCaRCGtOalSKG6ZDC4+x3AHZFqEYmiSE32hpg7hX4POdWQ1t4UKVR1PwYZOkVqsjd02ikUrWUj/VWUUFUwSC7y3OEVqcnerHWnUMSWTb8oAItFwSDRxd7hte50itRkX0oRWzb9MD1dGZoALAsFg0QXc4e31Bz8S93ftwg7pqK2bELbt68yFAFYJgoGia6bHd5Kd9ZZQ2ipVky/AmO55TZaNnffvYZkUuLul1EGmzcn04rAYAdgmSgYJLqsXTkhupyyhlCnAGlXw5YtXZXQVjc/2113Hc/cXIW77lrzksAahC6YyclaKbr2homCQXKRZfRFiC6nrCHUzZW8ywVDlqP4rD/bUu8bpHMQRRmNIwkFgxRWyKt0s9zCMfuVvMd1XE7Wo/isP9tS7+t2+wxCt5PEoWCQwoo9eijElbxZj+KzLnep93VT26B0O0kcCgYptCJ0MXRTQzdH8VmXu9T7si5jkLqdpP8UDNJXeXdfxF5/Ua+RGJahrxKGgkH6Ju/uizwvpLv66hf6tp5eFDWwpJgUDNI3/eq+yNoKKMKFdEVShG45KQcFg/RNP7ovutkBx+w+UR++DBIFg/RNP7ovutkBx+w+UR9+OeR9zqssFAzSV+1mDF3JL2a3O+BY3Sfqwy++MnT3FYWCQaIJ8YvZzXQaoXbSWZelPvxiU3dfdgoGiSbUL+ZyO+CQR4bdzJVU5m6KMteelbr7slMwSDSxfjFDHhlmnSupzN0UZa69G+ruyy5aMJjZicCtwJkkcwi/x92nYq1f8hfrFzNkAGWdKymPbopQR/nD1MWi7r5sYrYYbgH2uvtWMxsH1kRc91A0lcsgxi9myABa6eys/RLyKF9dLNIqSjCY2Y8Bm4F3Abj7LDAbY90wPE1lOSZkAK1kdtZ+CXmUry4WaRWrxfDTwA+B/2xmPwd8FbjG3f8uxsqHqaks+YnZTRH6KF9dLNKsUqu1v2VgSGa2EZgGznb3h83sFuA5d//NlvdtA7YBuPvrZ2fDNCqmpyts2TLK7CyMj8PevfNMTq785x4dHWV+fj5Ahf2lOsMqSp3T0xX27auweXOt7fe5KHUuR3WGMz4+DlBZ6XJitRieAJ5w94frj+8GPtz6JnffBeyqP6wdOHAgyMo3bIA77zx2jmHDhjlCLLparRKqxoalzoX0ep6kH3X2Q9Hq7LS9i1Lnhg3JH6Dt97kodS5HdYYzMTERZDlRgsHd/8bMHjez09z9r4DzgG/HWHdDGZrKy92QviznSboNsMb7zz+/cnRHl7cybW+R0GKOSroa+Hx9RNL3gHdHXHcprOT+vkUZdZV1h9qod/36BbZvP4G5uQq33JK07Irwc+m8lAyzaMHg7l8HNvby2aLs9Pqt1/v7dnN02+9tmWWH2lxvpVJjcbHC4mKF2dla6v15HrWvX79ApVJj1SoN4ZThU/grn4epSd88bHD9+gWmplYffX6pIYVZj25jbMsso2Wa6121Clatgkqlxvg4qfd3+rn6HW4zM2Ns334Ci4tJfTfc8OzAfudE2il8MAxbk77xs7XbgXc6T5J16GI3AdK84+1mR5xlTHxrvTfc8CyHDo1w/vnHs2HDXMf3bdp0JEq4NbbT4mLSojl0aCTo8kWKrvDBMIxXZXYbhr1enbt+/QI7d65NfaZ1x3vDDc8ePQfQeHzo0MiS61nuRH+neqvV41Kja9q9b+fOtX0/UBjG75xIs8IHwzBeldnLjqnbq3ObT/o2H3m3htJ99x1/9HGtBtdffyK1Gis+Ws86Sqz1fTF22sP4nRNpVvhggHIMNQ2pnzumxrbsdOTduuO94IIXefjhcYDUieLmz8QUa6c9bN85kWalCIZh0tyff/XVL/RtPZ2OvNvteH/mZ+ZbWhnxu1haz3Nopy3SPwqGAok5AmupI+/WHW/z40ZIxOxiGaaRaSJFoGAokNgjsHo58s7jaH3YRqaJ5G1V3gXIMUn3To2RkVrpR8PMzIyxc+daZmbGVrysQdouImWgFkNgK7n4alBGw4Tu+hmU7SJSFgqGgELsEAfhxGo/un4GYbuIlIW6kgJq3iHOzVWOTmkRQsiumX5T149IuanFEFC/Lr4q26icfnX9DMtkiiJ5K3QwtE7XUHT92iGWcVRO6K6fsoWjSJkVOhh27FiXeSdQlKPJpXaIvdQ4MzPGk0+uYmQkuXXjsHbNlDEcRcqq0MGQ9aY0ZTia7KXG5s+MjMDb336YrVsP53avhTxpYjuReAodDO1OXrbbwZbhaLKXGtOfqXHSSQtLhkLRw3ElNGRVJJ5CB8O11z6f6aY0ZTia7KXGbj5ThnBcKQ1ZFYmj0MHQbhK5djvLXo8mY3a99FJjN58pQziKSDlUarVa3jV0Utu/f3/bF0Ls0NNdL7Weul6q1SoHmu8sk7NO26VodXaiOsNSnWGVoc6JiQmAykqXE7XFYGYjwAzwpLtf2OtyQnQplKHrpdsAVFeLiIQQuyvpGuBR4Mcir/clit71Mugnk0WkuKJNiWFmrwJ+Abg11jqX0ui/v/ba5wu50+3n9BoiIkuJ2WL4XeDXgHX9XlGjC2b9+oUlb1xf5K6XordoRGRwRQkGM7sQeNrdv2pm5yzxvm3ANgB3p1qtdr2u6ekKl102ypEjsLgIq1bB6tXr2Lt3nsnJsCfaR0dHe6pxOdPTFf7yLyt84hOLHDwImzfXmJw8oefl9avO0FRnWKozrLLUGUKUUUlm9nHgncA8cBzJOYZ73P0dS3ys46ikpezcuZYdO9YdvSgMKoyM1Lj22ueD30O5H6MUQoyWalWG0RSgOkNTnWGVoc5SjUpy9+uA6wDqLYYPLhMKPWt0wdRqjRZD+6mfizp9RBlGS4nIYCv0BW69aL4orPkcAxybrRUo7IgfnVsQkbxFDwZ3fwh4qNfPZznSbz2p3Dr085JLDhf2qFxzAolI3krVYuh1bH9r9wwk/fdQzKPyIo+WEpHBV6pguPvuNRw5UqFW6+5Iv7V7ZuvWZPpqHZWLiLxUaYJhZmaM3bvXkAyiSu4nnPVIv1P3jAJBROSlShMMU1OrWVgAqFCp1Lj00s43rGlH3TMiItlEmxJjpZLuoKSlsHp1ja1bX8y7JBGRgVSaFoNG64iIxFGaYAB1B4mIxFCariQREYlDwSAiIikKBhERSVEwiIhIioJBRERSFAwiIpKiYBARkRQFQyAzM2Ps3LmWmZmxvEsREVmRUl3gVlS9TgcuIlJEajEE0Hy/h7m5ClNTq/MuSUSkZwqGAJon+CvijX9ERLqhrqQANMGfiAwSBUMgmuBPRAZFlGAws5OBzwKvABaBXe5+S4x1i4hId2KdY5gHPuDupwOTwFVmdkakdYuISBeiBIO7P+XuX6v/+3ngUeCkGOsWEZHuRB+VZGanAK8DHo69bhERWV6lVqtFW5mZrQX+DPhtd7+nzevbgG0A7v762dnZaLX1YnR0lPn5+bzLWJbqDEt1hqU6wxkfHweorHQ50YLBzMaALwH3u/tNGT5S279/f5+rWplqtcqBAwfyLmNZqjMs1RmW6gxnYmICAgRDlK4kM6sAnwYezRgKIiKSk1jXMZwNvBN4xMy+Xn/u1939vkjrFxGRjKIEg7v/DwI0b0REpP80V5KIiKQoGEREJEXBICIiKQoGERFJUTCIiEiKgkFERFIUDCIikqJgEBGRFAWDiIikKBhERCRFwSAiIikKBhERSVEwiIhIioJBRERSFAwiIpKiYBARkRQFg4iIpCgYREQkRcEgIiIpCgYREUkZjbUiM9sC3AKMALe6+42x1i0iItlFaTGY2QjwH4HzgTOAt5nZGTHWLSIi3YnVlXQW8F13/567zwJ3AhdHWreIiHQhVjCcBDze9PiJ+nMiIlIwsc4xVNo8V2t9wsy2AdsA3J2JiYl+17ViZagRVGdoqjMs1VkssVoMTwAnNz1+FbC/9U3uvsvdN7r7RjP7KkmgFPZPGWpUnaqz6H9UZ/AaVyxWi+F/A68xs1OBJ4HLgLdHWreIiHQhSovB3eeB9wL3A48mT/m3YqxbRES6E+06Bne/D7ivi4/s6lctAZWhRlCdoanOsFRnOEFqrNRqLzkHLCIiQ0xTYoiISEq0rqSG5abGMLPVwGeB1wPPAJe6+w/qr10HXAksAO9z9/tzrPPfAL8MzAM/BN7j7v+3/toC8Ej9rY+5+0U51vkuYAfJSX+AT7r7rfXXrgB+o/78v3P3z+RY583AufWHa4CXu/uJ9deibE8zuw24EHja3c9s83ql/jNcABwG3uXuX6u/FnNbLlfn5cCH6g9fAH7F3b9Rf+0HwPMkv0Pz7r4xxzrPAf4Y+H79qXvc/aP116JMoZOhxmuBy+sPR4HTgZ9w94ORt+XJJPvFVwCLwC53v6XlPcG+n1FbDBmnxrgSOOTuG4CbgX9f/+wZJKOZ/iGwBfi9+vLyqvMvgI3u/lrgbuB3ml570d1/vv6nn6GQdaqR3U31NELhZcB24I0kV6ZvN7P1edXp7u9v1AjsBO5pejnK9gRuJ/ludXI+8Jr6n23A70PcbZmxzu8Db6p/N3+Ll/Y7n1vfln3bkdXdztJ1Avz3pv/bRijEnEJnyRrdfUfT9/I64M/c/WDTW2Jty3ngA+5+OjAJXNVmmwT7fsbuSsoyNcbFQCPN7gbOqyfhxcCd7n7E3b8PfLe+vFzqdPevuPvh+sNpkmszYlvJVCNvBR5094Pufgh4kOV/iWPV+Tbgjj7V0pG77wMOLvGWi4HPunvN3aeBE83slcTdlsvW6e7/s14H5PfdzLI9O4k2hU6XNebyvQRw96caR//u/jzJ6M7W2SOCfT9jdyW1mxrjjZ3e4+7zZvYs8OP156dbPtuvaTWy1NnsSmBP0+PjzGyGJOVvdPcvhi8RyF7nvzCzzcB3gPe7++MdPpv79jSznwJOBf606elY23M5nbZZkad8af1u1oAHzKwGfMrd8x5ps8nMvkFywesH68PYu/396zszW0OyM31v09O5bEszOwV4HfBwy0vBvp+xWwyVNs+1Dovq9J4snw0l87rM7B3ARpJ+/IZX15uWbwd+18z+fvgSgWx1/glwSr1b4b9xrDVWyO1J0l14t7svND0Xa3supwjfzczM7FySYPhQ09Nnu/s/Iul2uKp+wJCXrwE/5e4/R9J92Aj8Im7PXwT+vKUbKfq2NLO1wB8Cv+ruz7W8HOz7GTsYskyNcfQ9ZjYKnEDS1Ms0rUbEOjGzNwPXAxe5+5HG8+6+v/7394CHSNI9lzrd/Zmm2v4TyUn9TJ+NWWeTy2hprkfcnsvp9HPE3JaZmNlrgVuBi939mcbzTdvyaeCP6F937LLc/Tl3f6H+7/uAMTOrUsDtydLfyyjb0szGSELh8+5+T5u3BPt+xu5KyjI1xr3AFcAUsBX4U3evmdm9wBfM7CZgguQEy//Kq04zex3wKWBL/YvReH49cNjdj9S/5GeTPjEdu85XuvtT9YcXkfRNQnIV+seaTkK9heTkWi511ms9DVhP8n/feC7m9lzOvcB7zexOkq6NZ939KTOLuS2XZWavJjl5/053/07T838PWOXuz9f//RbgozmViZm9Avjb+u/3WSQHqs8AP6JAU+iY2QnAm4B3ND0XdVvWz7N+GnjU3W/q8LZg38+owVA/Z9CYGmMEuM3dv2VmHwVm3P1ekh/+v5jZd0laCpfVP/stM3Pg2yR9zVe1dDfErnMHsBa4y8zg2DDK04FPmdkiyRf9Rnf/do51vs/MLiLZZgeBd9U/e9DMfotkpw3w0ZZmcuw6ITm5d6e7Nzdzo21PM7sDOAeomtkTJCM5xuo/wx+QXLl/AcnAh8PAu+uvRduWGev8tyTn5X6v/t1sDKX8SeCP6s+NAl9w97051rkV+BUzmwdeBC6r/9+3/b7kVCPAPwcecPe/a/po1G1JckD0TuARM/t6/blfB17dVGuw76eufBYRkRRd+SwiIikKBhERSVEwiIhIioJBRERSFAwiIpKiYBARkRQFg4iIpCgYREQkRcEgIiIp0e/gJlImZnY88H9I7pr1mubJEs3sVpJpBy539ztzKlEkOLUYRJbg7i+SzJ9zMvCvG8+b2cdJprS+WqEgg0ZzJYkso36ryW8ALwd+muRe3zcD2xu3oxQZJAoGkQzM7EKSmx59GfinwCfd/X35ViXSH+pKEsnA3b9Ecsex84DdwDX5ViTSPwoGkQwsmXj/5+sPn2+5Z4TIQFFXksgyzOwtJN1IfwLMAZcAP+vujy75QZGSUotBZAlm9kaS22T+OXA58BskQ1c/nmddIv2kYBDpwMxOB/4r8B3gl9z9iLv/NcntZy82s7NzLVCkTxQMIm2Y2auBB4BngfPd/bmmlz9Kco/i38mjNpF+0zkGERFJUYtBRERSFAwiIpKiYBARkRQFg4iIpCgYREQkRcEgIiIpCgYREUlRMIiISIqCQUREUhQMIiKS8v8BUYrBk1msCcwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X, y,'b.')\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0,2,0,15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Solution: \n",
    "2. When we want to build a model, how to prepare the data?\n",
    "    - training dataset, test dataset.\n",
    "    - in matrix format, dataframes\n",
    "    \n",
    "3. What's the analytical solution for linear regression? Under what circumstance can we or can't we use such solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closed form analytical solution: (Simplified version)\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "y & = X \\theta + \\epsilon \\\\\n",
    "X^{T} y & = X^TX\\theta \\\\\n",
    "(X^TX)^{-1}X^{T} y & = \\theta \\\\\n",
    "\\text{thus, }\\ \\hat{\\theta} & = (X^TX)^{-1}X^T y\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Exceptions: Inverse of $X^TX$ may not exist\n",
    "- X is non-invertible\n",
    "- X is singular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How to write python code for the analytical solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.74336758]\n",
      " [3.23182231]]\n"
     ]
    }
   ],
   "source": [
    "# analytical solution\n",
    "X_b = np.c_[np.ones((100,1)), X] # add extra column of all 1's to X\n",
    "theta_optimal = np.linalg.inv( X_b.T.dot(X_b) ).dot(X_b.T).dot(y) # perform the linear algebra inverse, transpose, and multiplication operations\n",
    "# theta_optimal should be close to m and b respectively.\n",
    "print(theta_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "5. gradient descent math derivation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple model equation:\n",
    "$$\\hat{Y} = mX + b$$\n",
    ", where gradient $m$ and intercept $b$ are the coefficients. We want to find the optimal coefficients so that the model has the least prediction error:\n",
    "$$\\text{Error} = \\hat{Y} - Y$$\n",
    ", where $\\hat{Y}$ is the predicted target value and $Y$ is the actual ground truth value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, we want to minimize the cost function:\n",
    "$$\\text{Cost_Function} = J_{m,b} = \\frac{1}{2} \\cdot \\frac{1}{n} \\sum_{i=1}^N (\\hat{Y}_i - Y_i)^2 = \\frac{1}{2} \\cdot \\frac{1}{n} \\sum_{i=1}^N (\\text{Error}_i)^2$$\n",
    "by _gradually_ change the coefficients to its optimum:\n",
    "$$m_{new} = m_{current} - \\delta m_{current}$$\n",
    "$$b_{new} = b_{current} - \\delta b_{current}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplify the $\\text{Cost_Function}$:\n",
    "\\begin{equation} \n",
    "\\begin{split}\n",
    "J_{m,b} & = \\frac{1}{2n} \\sum_{i=1}^N (\\hat{Y}_i - Y_i)^2 \\\\\n",
    "        & = \\frac{1}{2n} \\sum_{i=1}^N (\\text{Error}_i)^2 \\\\\n",
    "        & \\sim \\text{Error}^2\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivatives:\n",
    "\\begin{equation} \n",
    "\\begin{split}\n",
    "\\frac{\\partial J_{m,b}}{\\partial m} & = 2 \\cdot \\text{Error} \\cdot \\frac{\\partial}{\\partial m} \\text{Error}\\\\\n",
    "\\frac{\\partial J_{m,b}}{\\partial b} & = 2 \\cdot \\text{Error} \\cdot \\frac{\\partial}{\\partial b} \\text{Error} \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation} \n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial m} \\text{Error} & = \\frac{\\partial}{\\partial m}(\\hat{Y} - Y) = \\frac{\\partial}{\\partial m}(mX+b - Y) = X \\\\\n",
    "\\frac{\\partial}{\\partial b} \\text{Error} & = \\frac{\\partial}{\\partial b}(\\hat{Y} - Y) = \\frac{\\partial}{\\partial b}(mX+b - Y) = 1\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Chain rule:\n",
    "\\begin{equation} \n",
    "\\begin{split}\n",
    "\\frac{\\partial J_{m,b}}{\\partial m} & = 2 \\cdot \\text{Error} \\cdot X \\cdot \\text{Learning_Rate} = \\text{Error} \\cdot X \\cdot \\alpha \\\\\n",
    "\\frac{\\partial J_{m,b}}{\\partial b} & = 2 \\cdot \\text{Error} \\cdot 1 \\cdot \\text{Learning_Rate} = \\text{Error} \\cdot \\alpha\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Thus:\n",
    "$$m_1 = m_0 - \\text{Error} \\cdot X \\cdot \\alpha$$\n",
    "$$b_1 = b_0 - \\text{Error} \\cdot \\alpha$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalized model equation:\n",
    "$$\\hat{Y} = h_{\\theta}(X)$$\n",
    ", where $\\theta$ carries all parameters of the model. $h$ is the prediction function that takes $X$ to predict $\\hat{Y}$. And similar to the above, the error can be written as:\n",
    "$$\\text{Error} = \\hat{Y} - Y = \\sum_{i=1}^N (h_\\theta(x_i) - y_i)$$\n",
    ", where $\\hat{Y}$ is the predicted target value and $Y$ is the actual ground truth value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-write the cost function with $\\theta$, and $h$:\n",
    "\n",
    "$$ \\text{Cost_Function} = J_{\\theta} = \\frac{1}{2} \\cdot \\frac{1}{n} \\sum_{i=1}^N (\\text{Error}_i)^2 = \\frac{1}{2n} \\sum_{i=1}^N (h_\\theta(x_i) - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using numerical approach; gradient descent, we gradually change $\\theta$ to its optimal:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\theta_{new} &= \\theta_{current} - \\delta \\cdot \\theta_{current} \\\\\n",
    "         &= \\theta_{current} - \\underbrace{\\alpha}_{\\text{Learning_Rate}} \\cdot \\frac{\\partial}{\\partial \\theta} J_\\theta\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get $\\frac{\\partial}{\\partial \\theta}J_\\theta$:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial \\theta} J_\\theta &= \\frac{\\partial}{\\partial \\theta} \\frac{1}{2n} \\sum_{i=1}^N (h_\\theta(x_i) - y_i)^2 \\\\\n",
    "                                          &= \\frac{1}{2n} \\sum_{i=1}^N (h_\\theta(x_i) - y_i) \\cdot 2 \\cdot \\frac{\\partial}{\\partial \\theta_j} (h_\\theta(x_i) - y_i) \\\\\n",
    "                                          &= \\frac{1}{n} \\sum_{i=1}^N (h_\\theta(x_i) - y_i) \\cdot \\frac{\\partial}{\\partial \\theta_j} (h_\\theta(x_i) - y_i) \\\\\n",
    "                                          &= \\frac{1}{n} \\sum_{i=1}^N (h_\\theta(x_i) - y) \\cdot x_i \\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore,\n",
    "$$ \\theta_1 = \\theta_0 - \\alpha \\cdot \\frac{1}{n} \\sum_{i=1}^N (h_{\\theta_0}(x_i) - y_i) \\cdot x_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Python class \n",
    "6. Design and write a Linear Regression Python class that enables training and prediction. \n",
    "    - calculate_cost\n",
    "    - gradient_descent\n",
    "    - learning_rate, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLR:\n",
    "    \n",
    "    def __init__(self, op_algorithm):\n",
    "        self.coef_ = self.cost_ = None\n",
    "        self.op_algorithm = op_algorithm\n",
    "  \n",
    "    def calculate_cost(self,\n",
    "                       X: np.ndarray, y: np.ndarray):\n",
    "        predictions = self._predict(X)\n",
    "        self.cost_ = 1/(2 * len(y)) * np.sum(np.square(predictions - y))\n",
    "\n",
    "    # base gd\n",
    "    def gradient_descent(self, X: np.ndarray, y: np.ndarray,\n",
    "                         alpha: float, iterations: int):\n",
    "        for i in range(iterations):\n",
    "            predictions = self._predict(X)\n",
    "            error = predictions - y\n",
    "            self.coef_ = self.coef_ - alpha * (1/len(y)) * X.transpose().dot(error)\n",
    "            self.calculate_cost(X, y)\n",
    "    \n",
    "    # sgd:\n",
    "    def sgd(self, X: np.ndarray, y: np.ndarray, alpha: float, iterations: int):\n",
    "        sampling_count = len(y)\n",
    "        for i in range(iterations):\n",
    "            for i in range(sampling_count):\n",
    "                random_index = np.random.randint(0, sampling_count)\n",
    "                X_i = X[random_index, :].reshape(1, X.shape[1])\n",
    "                y_i = y[random_index].reshape(1, 1)\n",
    "                prediction_i = np.dot(X_i, self.coef_)\n",
    "                self.coef_ = self.coef_ - alpha * (1/len(y)) * (X_i.transpose().dot(prediction_i - y_i))\n",
    "            self.calculate_cost(X, y)\n",
    "    \n",
    "    # minibatch gd:\n",
    "    def minibatch_gd(self, X: np.ndarray, y: np.ndarray, alpha: float, iterations: int):\n",
    "        total_length = len(y)\n",
    "        # determine mini-batch size:\n",
    "        batch_size = np.random.randint(0, total_length/3)\n",
    "        if batch_size > 6 or batch_size == 0:\n",
    "            batch_size = 6\n",
    "            \n",
    "        for i in range(iterations):\n",
    "            for i in range(0, total_length, batch_size):\n",
    "                X_i = X[i:i+batch_size, :]\n",
    "                y_i = y[i:i+batch_size, :]\n",
    "                \n",
    "                prediction_i = np.dot(X_i, self.coef_)\n",
    "                self.coef_ = self.coef_ - alpha * (1/len(y)) * (X_i.transpose().dot(prediction_i - y_i))\n",
    "            self.calculate_cost(X, y)\n",
    "    \n",
    "    def add_intercept(self, X: np.ndarray):\n",
    "        return np.c_[X, np.ones((X.shape[0], 1))]\n",
    "            \n",
    "    def fit(self, X: np.ndarray,y: np.ndarray, \n",
    "            alpha:float = 0.01, iterations:int = 1000):\n",
    "        X = self.add_intercept(X)\n",
    "        self.coef_ = np.random.randn(X.shape[1],1)\n",
    "        if self.op_algorithm == 'gd':\n",
    "            self.gradient_descent(X, y, alpha, iterations)\n",
    "        elif self.op_algorithm == 'sgd':\n",
    "            self.sgd(X, y, alpha, iterations)\n",
    "        elif self.op_algorithm == 'minibatch':\n",
    "            self.minibatch_gd(X, y, alpha, iterations)\n",
    "\n",
    "    def _predict(self, X: np.ndarray):\n",
    "        return X.dot(self.coef_)\n",
    "    \n",
    "    def predict(self, X: np.ndarray):\n",
    "        X = self.add_intercept(X)\n",
    "        return self._predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from the previous section:\n",
    "m = 3\n",
    "b = 4\n",
    "X = 2 * np.random.rand(100,1)\n",
    "y = m * X + (b + np.random.randn(100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = [3.04246703], b = [4.01233047]\n",
      "cost = 0.5546021513052887\n"
     ]
    }
   ],
   "source": [
    "lr = SimpleLR('gd')\n",
    "lr.fit(X, y)\n",
    "\n",
    "theta_final = lr.coef_\n",
    "cost_final  = lr.cost_\n",
    "\n",
    "print(f'm = {theta_final[0]}, b = {theta_final[1]}')\n",
    "print(f'cost = {cost_final}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "In reality, when our training data is huge. We have to figure out how to train a model computational effectively. \n",
    "\n",
    "Difference:\n",
    "- Whole data set vs. single data point\n",
    "- SGD needs more iterations to reach minima but computationally effective. \n",
    "\n",
    "  \n",
    "7. Improve `SimpleLR` class with new `sgd` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgd:\n",
    "# run within previous class\n",
    "def sgd(self, X: np.ndarray, y: np.ndarray, alpha: float, iterations: int):\n",
    "    sampling_count = len(y)\n",
    "    for i in range(iterations):\n",
    "        for i in range(sampling_count):\n",
    "            random_index = np.random.randint(0, sampling_count)\n",
    "            X_i = X[random_index, :].reshape(1, X.shape[1])\n",
    "            y_i = y[random_index].reshape(1, 1)\n",
    "            prediction_i = np.dot(X_i, self.coef_)\n",
    "            self.coef_ = self.coef_ - alpha * (1/len(y)) * (X_i.transpose().dot(prediction_i - y_i))\n",
    "        self.calculate_cost(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = [2.85851072], b = [4.2450854]\n",
      "cost = 0.5496362335922517\n"
     ]
    }
   ],
   "source": [
    "# run a test:\n",
    "lr = SimpleLR('sgd')\n",
    "lr.fit(X, y)\n",
    "\n",
    "theta_final = lr.coef_\n",
    "cost_final  = lr.cost_\n",
    "\n",
    "print(f'm = {theta_final[0]}, b = {theta_final[1]}')\n",
    "print(f'cost = {cost_final}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent (Optional)\n",
    "Depends on the size of a mini-batch, the updates of coefficients can be less noisier than SGD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = [3.01582336], b = [4.04505173]\n",
      "cost = 0.5532115562203955\n"
     ]
    }
   ],
   "source": [
    "# run a test:\n",
    "lr = SimpleLR('minibatch')\n",
    "lr.fit(X, y)\n",
    "\n",
    "theta_final = lr.coef_\n",
    "cost_final  = lr.cost_\n",
    "\n",
    "print(f'm = {theta_final[0]}, b = {theta_final[1]}')\n",
    "print(f'cost = {cost_final}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "If we overfit our training data, our predictions may not generalize well to _new_ unseen data. Introduce regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J_\\theta = \\frac{1}{2N} \\sum_{i=1}^N (h_\\theta(x_i) - y_i)^2 + \\frac{1}{2}\\cdot \\lambda ||\\theta||^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
