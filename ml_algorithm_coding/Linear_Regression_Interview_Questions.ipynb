{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before asking the following questions, reserve 3~5 minutes for:\n",
    "- greet the candidate\n",
    "- briefly introduce ZG, ZO, myself, and work\n",
    "- ask the candidate for past MLE experiences\n",
    "\n",
    "After the technical questions:\n",
    "- ask 1 core value question (3~5 minutes)\n",
    "- leave 3~5 minutes for the candidates to ask questions.\n",
    "\n",
    "If the candidate does not know `numpy` or `python`, pseudo code is _okay_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Interview Questions (40~50 minutes)\n",
    "\n",
    "## Give a general use case:\n",
    "At Zillow, we have many prediction problems to solve. For example, we need to select features from a home to predict the price it may sell for. Let's assume we have a dataset $X$ that has many features of a home; i.e., number of bedrooms, year of constructions, etc. And we need to predict a target $Y$ which presents the selling price of the home. \n",
    "\n",
    "1. What are some of the fundamental models that we can try? How to choose?\n",
    "    - data type: numerial or categorical\n",
    "    - choose baseline model first\n",
    "\n",
    "Given that *Linear Regression* is commonly used for baseline trials, let's dive deep into the details of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data 1\n",
    "Present a simple 2-D $X$ that only has one feature, for illustration purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use(['ggplot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a generic expression of the linear regression model: $$y = mX + b$$\n",
    "let\n",
    "$$m = 3$$\n",
    "$$b = 4$$\n",
    "These $m, b$ are the ground truth of the coefficients of our model, but not necessarily the coefficients of our fitted model since we will be adding noise to the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 3\n",
    "b = 4\n",
    "X = 2 * np.random.rand(100,1)\n",
    "y = m * X + (b + np.random.randn(100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 0, 15]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X, y,'b.')\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0,2,0,15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Solution: \n",
    "2. When we want to build a model, how to prepare the data?\n",
    "    - training dataset, test dataset.\n",
    "    - in matrix format, dataframes\n",
    "    \n",
    "3. What's the analytical solution for linear regression? Under what circumstance can we or can't we use such solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closed form analytical solution: (Simplified version)\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "y & = X \\theta + \\epsilon \\\\\n",
    "X^{T} y & = X^TX\\theta \\\\\n",
    "(X^TX)^{-1}X^{T} y & = \\theta \\\\\n",
    "\\text{thus, }\\ \\hat{\\theta} & = (X^TX)^{-1}X^T y\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Exceptions: Inverse of $X^TX$ may not exist\n",
    "- X is non-invertible\n",
    "- X is singular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How to write python code for the analytical solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.29819703]\n",
      " [2.80255511]]\n"
     ]
    }
   ],
   "source": [
    "# analytical solution:\n",
    "#\n",
    "# add extra column of all 1's to X\n",
    "X_b = np.c_[np.ones((100,1)), X] \n",
    "# perform the linear algebra inverse, transpose, and multiplication operations\n",
    "theta_optimal = np.linalg.inv( X_b.T.dot(X_b) ).dot(X_b.T).dot(y) \n",
    "# theta_optimal should be close to m and b respectively.\n",
    "print(theta_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Solution: Gradient Descent\n",
    "5. gradient descent math derivation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple model equation:\n",
    "$$\\hat{Y} = mX + b$$\n",
    ", where gradient $m$ and intercept $b$ are the coefficients. We want to find the optimal coefficients so that the model has the least prediction error:\n",
    "$$\\text{Error} = \\hat{Y} - Y$$\n",
    ", where $\\hat{Y}$ is the predicted target value and $Y$ is the actual ground truth value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, we want to minimize the cost function:\n",
    "$$\\text{Cost_Function} = J_{m,b} = \\frac{1}{2} \\cdot \\frac{1}{n} \\sum_{i=1}^N (\\hat{Y}_i - Y_i)^2 = \\frac{1}{2} \\cdot \\frac{1}{n} \\sum_{i=1}^N (\\text{Error}_i)^2$$\n",
    "by _gradually_ change the coefficients to its optimum:\n",
    "$$m_{new} = m_{current} - \\delta m_{current}$$\n",
    "$$b_{new} = b_{current} - \\delta b_{current}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplify the $\\text{Cost_Function}$:\n",
    "\\begin{equation} \n",
    "\\begin{split}\n",
    "J_{m,b} & = \\frac{1}{2n} \\sum_{i=1}^N (\\hat{Y}_i - Y_i)^2 \\\\\n",
    "        & = \\frac{1}{2n} \\sum_{i=1}^N (\\text{Error}_i)^2 \\\\\n",
    "        & \\sim \\text{Error}^2\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivatives:\n",
    "\\begin{equation} \n",
    "\\begin{split}\n",
    "\\frac{\\partial J_{m,b}}{\\partial m} & = 2 \\cdot \\text{Error} \\cdot \\frac{\\partial}{\\partial m} \\text{Error}\\\\\n",
    "\\frac{\\partial J_{m,b}}{\\partial b} & = 2 \\cdot \\text{Error} \\cdot \\frac{\\partial}{\\partial b} \\text{Error} \n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation} \n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial m} \\text{Error} & = \\frac{\\partial}{\\partial m}(\\hat{Y} - Y) = \\frac{\\partial}{\\partial m}(mX+b - Y) = X \\\\\n",
    "\\frac{\\partial}{\\partial b} \\text{Error} & = \\frac{\\partial}{\\partial b}(\\hat{Y} - Y) = \\frac{\\partial}{\\partial b}(mX+b - Y) = 1\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Chain rule:\n",
    "\\begin{equation} \n",
    "\\begin{split}\n",
    "\\frac{\\partial J_{m,b}}{\\partial m} & = 2 \\cdot \\text{Error} \\cdot X \\cdot \\text{Learning_Rate} = \\text{Error} \\cdot X \\cdot \\alpha \\\\\n",
    "\\frac{\\partial J_{m,b}}{\\partial b} & = 2 \\cdot \\text{Error} \\cdot 1 \\cdot \\text{Learning_Rate} = \\text{Error} \\cdot \\alpha\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Thus:\n",
    "$$m_1 = m_0 - \\text{Error} \\cdot X \\cdot \\alpha$$\n",
    "$$b_1 = b_0 - \\text{Error} \\cdot \\alpha$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalized model equation:\n",
    "$$\\hat{Y} = h_{\\theta}(X)$$\n",
    ", where $\\theta$ carries all parameters of the model. $h$ is the prediction function that takes $X$ to predict $\\hat{Y}$. And similar to the above, the error can be written as:\n",
    "$$\\text{Error} = \\hat{Y} - Y = \\sum_{i=1}^N (h_\\theta(x_i) - y_i)$$\n",
    ", where $\\hat{Y}$ is the predicted target value and $Y$ is the actual ground truth value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-write the cost function with $\\theta$, and $h$:\n",
    "\n",
    "$$ \\text{Cost_Function} = J_{\\theta} = \\frac{1}{2} \\cdot \\frac{1}{n} \\sum_{i=1}^N (\\text{Error}_i)^2 = \\frac{1}{2n} \\sum_{i=1}^N (h_\\theta(x_i) - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using numerical approach; gradient descent, we gradually change $\\theta$ to its optimal:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\theta_{new} &= \\theta_{current} - \\delta \\cdot \\theta_{current} \\\\\n",
    "         &= \\theta_{current} - \\underbrace{\\alpha}_{\\text{Learning_Rate}} \\cdot \\frac{\\partial}{\\partial \\theta} J_\\theta\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get $\\frac{\\partial}{\\partial \\theta}J_\\theta$:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial \\theta} J_\\theta &= \\frac{\\partial}{\\partial \\theta} \\frac{1}{2n} \\sum_{i=1}^N (h_\\theta(x_i) - y_i)^2 \\\\\n",
    "                                          &= \\frac{1}{2n} \\sum_{i=1}^N (h_\\theta(x_i) - y_i) \\cdot 2 \\cdot \\frac{\\partial}{\\partial \\theta_j} (h_\\theta(x_i) - y_i) \\\\\n",
    "                                          &= \\frac{1}{n} \\sum_{i=1}^N (h_\\theta(x_i) - y_i) \\cdot \\frac{\\partial}{\\partial \\theta_j} (h_\\theta(x_i) - y_i) \\\\\n",
    "                                          &= \\frac{1}{n} \\sum_{i=1}^N (h_\\theta(x_i) - y) \\cdot x_i \\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore,\n",
    "$$ \\theta_1 = \\theta_0 - \\alpha \\cdot \\frac{1}{n} \\sum_{i=1}^N (h_{\\theta_0}(x_i) - y_i) \\cdot x_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Python class \n",
    "6. Design and write a Linear Regression Python class that enables training and prediction. \n",
    "    - calculate_cost\n",
    "    - gradient_descent\n",
    "    - learning_rate, iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p2 or p3 candidate, consider copy and past the following code skeleton to hackerrank:\n",
    "class SimpleLR:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # your code\n",
    "        pass \n",
    "        \n",
    "    def calculate_cost(self):\n",
    "        # your code        \n",
    "        pass\n",
    "\n",
    "    def gradient_descent(self):\n",
    "        # your code\n",
    "        pass \n",
    "    \n",
    "    def fit(self):\n",
    "        # your code\n",
    "        pass \n",
    "    \n",
    "    def predict(self):\n",
    "        # your code\n",
    "        pass\n",
    "        \n",
    "        \n",
    "# test case:\n",
    "m = 3\n",
    "b = 4\n",
    "X = 2 * np.random.rand(100,1)\n",
    "y = m * X + (b + np.random.randn(100,1))\n",
    "\n",
    "lr = SimpleLR()\n",
    "lr.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLR:\n",
    "    \n",
    "    def __init__(self, op_algorithm):\n",
    "        self.coef_ = self.cost_ = None\n",
    "        self.op_algorithm = op_algorithm\n",
    "  \n",
    "    def calculate_cost(self,\n",
    "                       X: np.ndarray, y: np.ndarray):\n",
    "        predictions = self._predict(X)\n",
    "        self.cost_ = 1/(2 * len(y)) * np.sum(np.square(predictions - y))\n",
    "\n",
    "    # base gd\n",
    "    def gradient_descent(self, X: np.ndarray, y: np.ndarray,\n",
    "                         alpha: float, iterations: int):\n",
    "        for i in range(iterations):\n",
    "            predictions = self._predict(X)\n",
    "            error = predictions - y\n",
    "            self.coef_ = self.coef_ - alpha * (1/len(y)) * X.transpose().dot(error)\n",
    "            self.calculate_cost(X, y)\n",
    "    \n",
    "    # sgd:\n",
    "    def sgd(self, X: np.ndarray, y: np.ndarray, \n",
    "            alpha: float, iterations: int):\n",
    "        sampling_count = len(y)\n",
    "        for i in range(iterations):\n",
    "            for i in range(sampling_count):\n",
    "                random_index = np.random.randint(0, sampling_count)\n",
    "                X_i = X[random_index, :].reshape(1, X.shape[1])\n",
    "                y_i = y[random_index].reshape(1, 1)\n",
    "                prediction_i = np.dot(X_i, self.coef_)\n",
    "                error_i = prediction_i - y_i\n",
    "                self.coef_ = self.coef_ - alpha * (1/len(y)) * (X_i.transpose().dot(error_i))\n",
    "            self.calculate_cost(X, y)\n",
    "    \n",
    "    # minibatch gd:\n",
    "    def minibatch_gd(self, X: np.ndarray, y: np.ndarray, \n",
    "                     alpha: float, iterations: int):\n",
    "        total_length = len(y)\n",
    "        # determine mini-batch size:\n",
    "        batch_size = np.random.randint(0, total_length/3)\n",
    "        if batch_size > 6 or batch_size == 0:\n",
    "            batch_size = 6\n",
    "            \n",
    "        for i in range(iterations):\n",
    "            for i in range(0, total_length, batch_size):\n",
    "                X_i = X[i:i+batch_size, :]\n",
    "                y_i = y[i:i+batch_size, :]\n",
    "                prediction_i = np.dot(X_i, self.coef_)\n",
    "                error_i = prediction_i - y_i\n",
    "                self.coef_ = self.coef_ - alpha * (1/len(y)) * (X_i.transpose().dot(error_i))\n",
    "            self.calculate_cost(X, y)\n",
    "    \n",
    "    def add_intercept(self, X: np.ndarray):\n",
    "        return np.c_[X, np.ones((X.shape[0], 1))]\n",
    "            \n",
    "    def fit(self, X: np.ndarray,y: np.ndarray, \n",
    "            alpha:float = 0.01, \n",
    "            iterations:int = 1000):\n",
    "        X = self.add_intercept(X)\n",
    "        self.coef_ = np.random.randn(X.shape[1],1)\n",
    "        if self.op_algorithm == 'gd':\n",
    "            self.gradient_descent(X, y, alpha, iterations)\n",
    "        elif self.op_algorithm == 'sgd':\n",
    "            self.sgd(X, y, alpha, iterations)\n",
    "        elif self.op_algorithm == 'minibatch':\n",
    "            self.minibatch_gd(X, y, alpha, iterations)\n",
    "\n",
    "    def _predict(self, X: np.ndarray):\n",
    "        return X.dot(self.coef_)\n",
    "    \n",
    "    def predict(self, X: np.ndarray):\n",
    "        X = self.add_intercept(X)\n",
    "        return self._predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from the previous section:\n",
    "m = 3\n",
    "b = 4\n",
    "X = 2 * np.random.rand(100,1)\n",
    "y = m * X + (b + np.random.randn(100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = [3.29560284], b = [3.41106183]\n",
      "cost = 0.3766702117418918\n"
     ]
    }
   ],
   "source": [
    "lr = SimpleLR('gd')\n",
    "lr.fit(X, y)\n",
    "\n",
    "theta_final = lr.coef_\n",
    "cost_final  = lr.cost_\n",
    "\n",
    "print(f'm = {theta_final[0]}, b = {theta_final[1]}')\n",
    "print(f'cost = {cost_final}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "In reality, when our training data is huge. We have to figure out how to train a model computational effectively. \n",
    "\n",
    "Difference:\n",
    "- Whole data set vs. single data point\n",
    "- SGD needs more iterations to reach minima but computationally effective. \n",
    "\n",
    "  \n",
    "7. Improve `SimpleLR` class with new `sgd` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgd:\n",
    "# run within previous class\n",
    "def sgd(self, X: np.ndarray, y: np.ndarray, alpha: float, iterations: int):\n",
    "    sampling_count = len(y)\n",
    "    for i in range(iterations):\n",
    "        for i in range(sampling_count):\n",
    "            random_index = np.random.randint(0, sampling_count)\n",
    "            X_i = X[random_index, :].reshape(1, X.shape[1])\n",
    "            y_i = y[random_index].reshape(1, 1)\n",
    "            prediction_i = np.dot(X_i, self.coef_)\n",
    "            error_i = prediction_i - y_i\n",
    "            self.coef_ = self.coef_ - alpha * (1/len(y)) * (X_i.transpose().dot(error_i))\n",
    "        self.calculate_cost(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = [3.32486779], b = [3.3860652]\n",
      "cost = 0.3792388164308347\n"
     ]
    }
   ],
   "source": [
    "# run a test:\n",
    "lr = SimpleLR('sgd')\n",
    "lr.fit(X, y)\n",
    "\n",
    "theta_final = lr.coef_\n",
    "cost_final  = lr.cost_\n",
    "\n",
    "print(f'm = {theta_final[0]}, b = {theta_final[1]}')\n",
    "print(f'cost = {cost_final}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent (Optional)\n",
    "Depends on the size of a mini-batch, the updates of coefficients can be less noisier than SGD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = [3.1083427], b = [3.64118767]\n",
      "cost = 0.36556005768866656\n"
     ]
    }
   ],
   "source": [
    "# run a test:\n",
    "lr = SimpleLR('minibatch')\n",
    "lr.fit(X, y)\n",
    "\n",
    "theta_final = lr.coef_\n",
    "cost_final  = lr.cost_\n",
    "\n",
    "print(f'm = {theta_final[0]}, b = {theta_final[1]}')\n",
    "print(f'cost = {cost_final}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected value of mini-batch in SGD\n",
    "Prove that the expected value of a mini-batch in SGD equals to the true empirical gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical gradient: \n",
    "$$\\nabla J(X) = \\frac{1}{n} \\sum_{i=1}^n \\nabla \\text{Loss}(h(x_i)-y_i)$$\n",
    "\n",
    "For a mini-batch, the expected value is:\n",
    "$$E \\big [ \\frac{1}{m} \\sum_{i=1}^m \\nabla \\text{Loss}(h(x_i)-y_i) \\big ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that:\n",
    "$$E[X] = \\sum_x x \\times P(X=x)$$\n",
    "and from linearity of expectation, we know:\n",
    "$$E[X+Y] = E[X] + E[Y]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, \n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "E \\big [ \\frac{1}{m} \\sum_{i=1}^m \\nabla \\text{Loss}(h(x_i)-y_i)) \\big ] & = \\frac{1}{m} \\sum_{i=1}^m E \\big [\\nabla \\text{Loss}(h(x_i)-y_i) \\big ] \\\\\n",
    "&= \\frac{1}{m} \\sum_{i=1}^m \\left \\{ \\sum_j^n \\nabla \\text{Loss}(h(x_i)-y_i) \\times P(j=i)  \\right \\} \\\\\n",
    "&\\text{Given that } P(j=i) \\text{ is chosen uniformly at random,} \\\\\n",
    "&= \\frac{1}{m} \\sum_{i=1}^m \\left \\{ \\frac{1}{n} \\sum_j^n \\nabla \\text{Loss}(h(x_i)-y_i) \\right \\} \\\\\n",
    "&= \\frac{1}{m} \\sum_{i=1}^m \\nabla J(X) \\\\\n",
    "&= \\nabla J(X) \\\\\n",
    "\\text{thus, we have:}& \\\\\n",
    "\\text{the expected value of a mini-batch in SGD } &=\\text{true empirical gradient }  \\\\\n",
    "E \\big [ \\frac{1}{m} \\sum_{i=1}^m \\nabla \\text{Loss}(h(x_i)-y_i)) \\big ] &= \\nabla J(X) \\ \\Box\n",
    "\\end{split}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use MLE to explain the solution (Optional):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given general form of linear regression: \n",
    "$$Y = X\\beta + \\epsilon, \\  \\epsilon \\sim\\mathit{N}(0, \\Sigma)$$\n",
    "Thus:\n",
    "$$Y \\sim\\mathit{N}(X\\beta, \\Sigma)$$\n",
    "$$\\text{pdf}: \\Longrightarrow L(\\beta; X, Y) = \\frac{1}{\\sqrt{2\\pi |\\Sigma|}} EXP\\left\\{-\\frac{1}{2} (Y-X\\beta)^T \\Sigma^{-1}(Y-X\\beta)\\right\\}$$\n",
    "Therefore:\n",
    "$$\\hat{\\beta} = \\underset{\\beta}{\\mathrm{argmax}} L(\\beta; X, Y) = \\underset{\\beta}{\\mathrm{argmax}} \\ \\mathit{log} \\ {L(\\beta; X, Y)}$$\n",
    "$$\\Sigma \\ \\text{constant to} \\ \\hat{\\beta} \\propto \\underset{\\beta}{\\mathrm{argmax}}\\left\\{-\\frac{1}{2} (Y-X\\beta)^T \\Sigma^{-1}(Y-X\\beta)\\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "If we overfit our training data, our predictions may not generalize well to _new_ unseen data. Introduce regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J_\\theta = \\frac{1}{2N} \\sum_{i=1}^N (h_\\theta(x_i) - y_i)^2 + \\frac{1}{2}\\cdot \\lambda ||\\theta||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "8. Explain cross validation and why do we need cross validation.\n",
    "9. Write cross validation split function.\n",
    "10. Implement k-fold cross validation into the SimpleLR class\n",
    "11. Find *bug* in the following `wrong_cross_validation_split` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    "\n",
    "def wrong_cross_validation_split(data: np.ndarray, \n",
    "                                 folds: int = 3):\n",
    "    data_split = list()\n",
    "    \n",
    "    fold_size = int(data.shape[0] / folds)\n",
    "\n",
    "    for i in range(folds):\n",
    "        _x = np.empty([1, data.shape[1]])\n",
    "\n",
    "        while _x.shape[0] < fold_size:\n",
    "            index = randrange(data.shape[0])\n",
    "            _x = np.append(_x, \n",
    "                           data[index].reshape(1, data.shape[1]), axis=0)\n",
    "        data_split.append(_x)\n",
    "        \n",
    "    return data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
